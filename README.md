Technologies: Python, TensorFlow, OpenCV, Keras, Deep Learning

Key Contributions:

Developed a deep learning model using TensorFlow and Keras for recognizing and translating sign language gestures into text and speech.

Implemented OpenCV for real-time video capture and image processing, enabling smooth gesture recognition.

Trained the model on a custom dataset of hand gestures, achieving a recognition accuracy of 90% for commonly used signs.

Built a user-friendly interface using PyQt for non-sign language users to communicate seamlessly with sign language users.

Outcome: Facilitated real-time communication between hearing and hearing-impaired individuals, contributing to a more inclusive environment.
